# stages.py

import json
import re
import os
import ast
import inspect
import asyncio
import importlib
import random
import math
import hashlib
import logging
import difflib
import shutil
import threading
import concurrent.futures
import io
import contextlib
import textwrap
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Callable

import numpy as np
from ollama import chat, embed
from tools import Tools
from context import (
    ContextObject,
    default_clock,
    HybridContextRepository,
    MemoryManager,
)
# if your stage refers to other parts of your assembler you may need to
# import them too; adjust as needed.
# ‚Äî‚Äî‚Äî NEW helper ‚Äî‚Äî‚Äî

def _utc_iso() -> str:
    """UTC timestamp ending with 'Z' (e.g. 2025-07-07T18:04:31.123456Z)."""
    from datetime import datetime
    return datetime.utcnow().isoformat() + "Z"

def _stamp(ctx, state):
    ctx.metadata.setdefault("conversation_id", state["conversation_id"])
    ctx.metadata.setdefault("user_id",         state["user_id"])

def squash_narrative(lines, keep_last=5, max_len=160):
    """
    lines : iterable[str]   (chronological order)
    Returns a compact list with at most `keep_last` recent items plus a
    single summary for the elided middle.
    """
    total = len(lines)
    if total <= keep_last:
        pool = lines
    else:
        pool = ["‚Ä¶ (%d earlier entries)" % (total - keep_last)] + list(lines[-keep_last:])

    out = []
    for ln in pool:
        # cut at first sentence end ‚â§ max_len
        sent_end = re.search(r"[.!?]\s", ln)
        clip_at  = sent_end.end() if sent_end and sent_end.end() <= max_len else max_len
        out.append(textwrap.shorten(ln, clip_at, placeholder="‚Ä¶"))
    return out

def trim_snip(snip, max_words=100, ctx_id=""):
    words = snip.split()
    if len(words) <= max_words:
        return snip
    # find nearest sentence end before the cut
    cut = " ".join(words[:max_words])
    m   = re.search(r"[.!?]\s[^.!?]*?$", cut)
    cut = cut[:m.end()] if m else cut
    return f"{cut} ‚Ä¶ [extended {len(snip)-len(cut)} chars, search {ctx_id}]"

# -----------------------------------------------------------------------
# Pretty debug snapshot for _stage10_assemble_and_infer
# -----------------------------------------------------------------------
def _dump_ai_state(state: dict[str, Any]) -> None:
    from textwrap import shorten

    # ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _crop(val: str, ln: int = 60) -> str:
        return shorten(str(val), width=ln, placeholder="‚Ä¶")

    # Core scalars -------------------------------------------------------
    plan_out  = _crop(state.get("plan_output", "(none)"), 120)
    tc_ctx_id = getattr(state.get("tc_ctx"), "context_id", None)
    wm_ids    = state.get("wm_ids", [])
    rec_ids   = state.get("recent_ids", [])

    # Tool-ctx table -----------------------------------------------------
    tool_rows = []
    for t in state.get("tool_ctxs", []):
        stage  = t.stage_id
        cid    = t.context_id
        output = _crop(t.metadata.get("output", "(no output)"))
        tool_rows.append(f"‚îÇ {stage:<24} ‚îÇ {cid:<36} ‚îÇ {output}")

    tool_table = (
        "‚îÇ stage_id                 ‚îÇ context_id                          ‚îÇ output\n"
        "‚îú" + "‚îÄ"*26 + "‚îº" + "‚îÄ"*38 + "‚îº" + "‚îÄ"*62 + "\n"
        + "\n".join(tool_rows or ["‚îÇ (none)                    ‚îÇ (n/a)                               ‚îÇ"])
    )

    # Final block --------------------------------------------------------
    block = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê  ASSEMBLE & INFER  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë State keys : {_crop(list(state.keys()), 100)}{' '*(53-len(_crop(list(state.keys()),100)))}‚ïë
‚ïë plan_out   : {plan_out:<53}‚ïë
‚ïë tc_ctx_id  : {tc_ctx_id or '(none)':<53}‚ïë
‚ïë wm_ids     : {_crop(wm_ids, 100):<53}‚ïë
‚ïë recent_ids : {_crop(rec_ids, 100):<53}‚ïë
‚ïü‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TOOL CTXS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢
{tool_table}
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
    print(block.strip("\n"))
import uuid
def _ensure_ids(ctx, conv_id, user_id):
    ctx.metadata.setdefault("conversation_id", conv_id)
    ctx.metadata.setdefault("user_id",         user_id)

def _stage1_record_input(self, user_text: str, state: Dict[str, Any]) -> ContextObject:
    ctx = ContextObject.make_segment("user_input", [], tags=["user_input"])
    ctx.summary = user_text
    ctx.stage_id = "user_input"

    # üîë  inject routing metadata so Stage 3 can find it later
    ctx.metadata.update({
        "conversation_id": state["conversation_id"],
        "user_id":         state["user_id"],
    })

    ctx.touch()
    self.repo.save(ctx)
    return ctx

def _stage2_load_system_prompts(self) -> List[ContextObject]:
    """
    Load each static system-prompt ContextObject (never evicted)
    and return them as a list, in label order.
    """
    # 1) make sure we‚Äôve seeded/updated the on-disk prompt artifacts
    self._seed_static_prompts()

    # 2) for each prompt label we know about, grab the newest ContextObject
    prompts: List[ContextObject] = []
    for label in self.system_prompts.keys():
        # find all saved prompts with this semantic_label
        candidates = sorted(
            self.repo.query(lambda c: c.semantic_label == label),
            key=lambda c: c.timestamp,
            reverse=True
        )
        if not candidates:
            # (shouldn‚Äôt happen, since seed just inserted it)
            continue
        prompts.append(candidates[0])

    # 3) Persist & index them so retrieval/integrator never prunes them
    self._persist_and_index(prompts)

    return prompts

def _stage3_retrieve_and_merge_context(
    self,
    user_text: str,
    user_ctx: "ContextObject | None",
    sys_ctx: "ContextObject | List[ContextObject] | None",
    extra_ctx: List["ContextObject"] | None = None,
    recall_ids: List[str] | None = None,
) -> Dict[str, Any]:
    """
    Build the conversation memory window for downstream stages.

    Returns:
      merged        ‚Üí  List[ContextObject]  (entire ordered context window)
      merged_ids    ‚Üí  List[str]            (ids of ^)
      wm_ids        ‚Üí  List[str]            (ids of working-memory slice)
      history       ‚Üí  last 8 user/assistant ContextObjects
      tools         ‚Üí  recent tool-output ContextObjects
      semantic      ‚Üí  semantic recall ContextObjects
      assoc         ‚Üí  associative recall ContextObjects
    """
    from datetime import datetime, timedelta
    now = datetime.utcnow()

    # If there's no user context, nothing to build
    if user_ctx is None:
        return {
            "merged":       [],
            "merged_ids":   [],
            "wm_ids":       [],
            "history":      [],
            "tools":        [],
            "semantic":     [],
            "assoc":        [],
        }

    # Helpers
    def _to_dt(ts: str) -> datetime:
        try:
            return datetime.fromisoformat(ts.rstrip("Z"))
        except Exception:
            return now

    def _ensure_list(x):
        if x is None:
            return []
        return x if isinstance(x, list) else [x]

    # Flatten inputs
    user_list  = _ensure_list(user_ctx)
    sys_list   = _ensure_list(sys_ctx)
    extra_list = extra_ctx or []

    # Pull conversation & user IDs from the first user_ctx
    primary = user_list[0]
    conv_id = (
        primary.metadata.get("conversationid")
        or primary.metadata.get("conversation_id")
    )
    user_id = primary.metadata.get("user_id")

    # 1. Load raw conversation segments
    segs = [
        c for c in self.repo.query(lambda c:
            c.metadata.get("conversationid") == conv_id
            or c.metadata.get("conversation_id") == conv_id
        )
        if c.domain == "segment"
        and c.metadata.get("user_id") in (user_id, None)
        and c.semantic_label in ("user_input", "assistant")
    ]

    # 1a. Prepend extra_ctx
    seen_ids = {c.context_id for c in segs}
    for c in extra_list:
        if c.context_id not in seen_ids:
            segs.append(c)
            seen_ids.add(c.context_id)

    # 1b. Include explicit recall_ids
    if recall_ids:
        for rid in recall_ids:
            try:
                c = self.repo.get(rid)
                if c.context_id not in seen_ids:
                    segs.append(c)
                    seen_ids.add(c.context_id)
            except KeyError:
                pass

    segs.sort(key=lambda c: _to_dt(c.timestamp))

    # 2. Working-memory & short-term slices
    WM_TURNS   = 20
    ST_MINUTES = 120
    wm_slice   = segs[-WM_TURNS:]
    cutoff     = now - timedelta(minutes=ST_MINUTES)
    st_slice   = [c for c in segs if _to_dt(c.timestamp) >= cutoff]

    # 3. Semantic & associative recall
    semantic, assoc = [], []
    if self.rl.should_run("semantic_retrieval", 0.0):
        hits = self.engine.query(
            stage_id="semantic_retrieval",
            similarity_to=user_text,
            exclude_tags=self.STAGES + ["tool_schema", "tool_output"],
            top_k=self.top_k,
        )
        semantic = [
            h for h in hits
            if (h.metadata.get("conversationid") == conv_id
                or h.metadata.get("conversation_id") == conv_id)
        ]

    if self.rl.should_run("memory_retrieval", 0.0):
        seeds  = [primary.context_id]
        scores = self.memman.spread_activation(seeds, hops=3, decay=0.7)
        for cid in sorted(scores, key=scores.get, reverse=True)[: self.top_k]:
            try:
                c = self.repo.get(cid)
                if (c.metadata.get("conversationid") == conv_id
                    or c.metadata.get("conversation_id") == conv_id):
                    c.retrieval_score = scores[cid]
                    assoc.append(c)
            except KeyError:
                pass

    # 4. Recent tool outputs
    recent_tools = [
        c for c in self.repo.query(lambda c:
            c.component == "tool_output"
            and ((c.metadata.get("conversationid") == conv_id)
                 or (c.metadata.get("conversation_id") == conv_id))
        )
    ]
    recent_tools.sort(key=lambda c: _to_dt(c.timestamp))
    recent_tools = recent_tools[-self.top_k:]

    # 5. Prefix summaries once
    def _prefix(ctx):
        if not getattr(ctx, "summary", None):
            return
        if ctx.summary.lstrip().startswith(("User:", "Assistant:")):
            return
        role = "Assistant" if ctx.semantic_label == "assistant" else "User"
        ctx.summary = f"{role}: {ctx.summary or ''}"

    for c in sys_list + user_list + segs + semantic + assoc + recent_tools:
        _prefix(c)

    # 6. Assemble merged list, de-duped
    merged, seen = [], set()
    def _add(objs):
        for c in objs:
            cid = c.context_id
            if cid not in seen:
                merged.append(c)
                seen.add(cid)

    _add(sys_list)
    _add(user_list)
    _add(wm_slice)
    _add(st_slice)
    _add(semantic)
    _add(assoc)
    _add(recent_tools)

    # 7. Build return dict
    result = {
        "merged":      merged,
        "merged_ids":  [c.context_id for c in merged],
        "wm_ids":      [c.context_id for c in wm_slice],
        "history":     merged[-8:],
        "tools":       recent_tools,
        "semantic":    semantic,
        "assoc":       assoc,
    }

    # 8. Debug logging
    self._print_stage_context("retrieve_and_merge_context", {
        "total_segments": len(segs),
        "merged_ids":     result["merged_ids"][:12],
        "wm_ids":         result["wm_ids"],
        "semantic_ids":   [c.context_id for c in semantic],
        "assoc_ids":      [c.context_id for c in assoc],
        "tool_ids":       [c.context_id for c in recent_tools],
    })

    return result




def _stage4_intent_clarification(
    self,
    user_text: str,
    state: Dict[str, Any],
    *,
    on_token: Callable[[str],None] | None = None,
    ) -> "ContextObject":
    """
    Ask the Clarifier model to restate / expand the user's intent.

    Prompt includes:
      ‚Ä¢ All post-tool dialogue (otherwise last 8 turns)
      ‚Ä¢ The 8 turns *preceding* the current message (contextual glue)
      ‚Ä¢ Last 3 tool outputs (truncated)
      ‚Ä¢ Short semantic / associative / tool context snippets

    Returned JSON must contain:
        { "keywords": [], "notes": "", "debug_notes": [] }
    """
    import json, textwrap
    from context import ContextObject

    # ------------------------------------------------------------------ #
    # 0) Guards & shorthands                                             #
    # ------------------------------------------------------------------ #
    state          = state or {}
    merged         = state.get("merged", [])
    tool_ctxs      = state.get("tool_ctxs", [])
    semantic_ctxs  = state.get("semantic", [])
    assoc_ctxs     = state.get("assoc", [])
    tool_refs      = state.get("tools", [])

    # keep dialog ContextObjects in chronological order
    hist = [
        c for c in merged
        if c.semantic_label in ("user_input", "assistant")
    ]
    hist.sort(key=lambda c: c.timestamp)

    # ------------------------------------------------------------------ #
    # 1) Build ‚Äúrecent dialogue‚Äù (post-tool or fallback)                 #
    # ------------------------------------------------------------------ #
    last_tool_ts = max((tc.timestamp for tc in tool_ctxs), default=None)
    dialogue: list[str] = []

    for c in hist:
        if last_tool_ts and c.timestamp <= last_tool_ts:
            # skip dialogue that happened *before* the last tool run
            continue
        role  = "User" if c.semantic_label == "user_input" else "Assistant"
        text  = c.summary or c.metadata.get("text", "")
        dialogue.append(f"{role}: {text}")

    # Fallback ‚Üí last 8 turns if post-tool block ended up empty
    if not dialogue:
        for c in hist[-8:]:
            role = "User" if c.semantic_label == "user_input" else "Assistant"
            dialogue.append(f"{role}: {c.summary or c.metadata.get('text', '')}")

    # Hard truncate dialog block
    dialog_block = "\n".join(dialogue)[-1500:] or "(none)"

    # ------------------------------------------------------------------ #
    # 2) Previous-turn snippet (8 lines before the current message)      #
    # ------------------------------------------------------------------ #
    prev_lines: list[str] = []
    if len(hist) >= 2:                     # guarantee at least one earlier turn
        for c in hist[-9:-1]:
            role = "User" if c.semantic_label == "user_input" else "Assistant"
            prev_lines.append(f"{role}: {c.summary or c.metadata.get('text', '')}")
    prev_block = "\n".join(prev_lines) if prev_lines else "(none)"

    # ------------------------------------------------------------------ #
    # 3) Last 3 tool outputs                                             #
    # ------------------------------------------------------------------ #
    tool_lines: list[str] = []
    for tc in sorted(tool_ctxs, key=lambda c: c.timestamp)[-3:]:
        payload = tc.metadata.get("output") or tc.metadata.get("exception") or ""
        try:
            blob = (
                payload
                if isinstance(payload, str)
                else json.dumps(payload, ensure_ascii=False)
            )
        except Exception:
            blob = repr(payload)
        if len(blob) > 950:
            blob = blob[:950] + " ‚Ä¶"
        tool_lines.append(f"[{tc.stage_id}] {blob}")
    tools_block = "\n".join(tool_lines) if tool_lines else "(none)"

    # ------------------------------------------------------------------ #
    # 4) Semantic / associative / tool reference snippets                #
    # ------------------------------------------------------------------ #
    def _first_n(ctxs, n=3):
        out = []
        for c in ctxs[:n]:
            short = (c.summary or "")[:120].replace("\n", " ")
            out.append(f"‚Ä¢ {short}  (id={c.context_id[:8]})")
        return out

    semantic_block = "\n".join(_first_n(semantic_ctxs)) or "(none)"
    assoc_block    = "\n".join(_first_n(assoc_ctxs))    or "(none)"
    tools_block2   = "\n".join(_first_n(tool_refs))      or "(none)"

    # ------------------------------------------------------------------ #
    # 5) Assemble full system/context prompt                             #
    # ------------------------------------------------------------------ #
    clar_sys = self.clarifier_prompt
    full_ctx = textwrap.dedent(f"""
        ### Recent Dialogue ###
        {dialog_block}

        ### Previous User / Assistant Turns ###
        {prev_block}

        ### Recent Tool Outputs ###
        {tools_block}

        ### Retrieved Semantic Context ###
        {semantic_block}

        ### Retrieved Associative Context ###
        {assoc_block}

        ### Tool Reference Context ###
        {tools_block2}

        ### Current User Query ###
        {user_text}
    """).strip()

    # cap entire context to 4 kB to protect model window
    MAX_PROMPT_CHARS = 4096
    if len(full_ctx) > MAX_PROMPT_CHARS:
        full_ctx = full_ctx[-MAX_PROMPT_CHARS:]

    # ------------------------------------------------------------------ #
    # 6) Call the Clarifier model                                        #
    # ------------------------------------------------------------------ #
    msgs = [
        {"role": "system", "content": clar_sys},
        {"role": "system", "content": full_ctx},
        {"role": "user",   "content": user_text},
    ]
    out = self._stream_and_capture(
        self.primary_model,                       # ‚Üê use primary model
        msgs,
        tag="[Clarifier]",
        images=state.get("images"),
    ).strip()

    # ------------------------------------------------------------------ #
    # 7) Parse / repair JSON                                             #
    # ------------------------------------------------------------------ #
    def _as_json(raw: str) -> dict | None:
        try:
            data = json.loads(raw)
            if (
                isinstance(data, dict)
                and "keywords" in data
                and "notes"    in data
            ):
                return data
        except Exception:
            pass
        return None

    clar = _as_json(out)


    # Final fallback: wrap raw text
    if clar is None:
        clar = {
            "keywords": [],
            "notes": out,
            "debug_notes": dialogue[-8:],
        }

    # guarantee debug_notes
    clar.setdefault("debug_notes", dialogue[-8:])

    # ------------------------------------------------------------------ #
    # 8) Persist Clarifier Context                                       #
    # ------------------------------------------------------------------ #
    input_refs = [state["user_ctx"].context_id] if state.get("user_ctx") else []
    clar_ctx = ContextObject.make_stage(
        "intent_clarification",
        input_refs=input_refs,
        output=clar,
    )
    clar_ctx.metadata.update(clar)            # keep keywords / notes
    clar_ctx.stage_id       = "intent_clarification"
    clar_ctx.semantic_label = "intent_clarification"
    clar_ctx.tags.append("clarifier")

    # propagate conversation / user ids if available
    if state.get("user_ctx"):
        clar_ctx.metadata.update(
            {
                "conversation_id": state["user_ctx"].metadata["conversation_id"],
                "user_id": state["user_ctx"].metadata["user_id"],
            }
        )

    clar_ctx.summary = clar.get("notes", "")[:250]

    clar_ctx.touch()
    self.repo.save(clar_ctx)
    # embed for later retrieval
    #self.memman.register_relationships(clar_ctx, self.embed_text)

    return clar_ctx

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# _stage5_external_knowledge   (upgraded)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _stage5_external_knowledge(
    self,
    clar_ctx: "ContextObject",
    state: Dict[str, Any] | None = None,
) -> "ContextObject":
    """
    Build a ranked ‚Äúexternal knowledge‚Äù ContextObject for the planner.

    Signal sources (in trust order):

      ‚Ä¢ Recent dialogue turns            (last 6)
      ‚Ä¢ Recent tool outputs              (last 6)
      ‚Ä¢ Semantic recalls                 (saved in state["semantic"])
      ‚Ä¢ Associative holographic recall   (MemoryManager.holographic_recall)
      ‚Ä¢ Fresh similarity hits            (engine.query, recency‚Äëboosted)

    Score  =  0.55‚ÄØ¬∑‚ÄØsimilarity   +   0.25‚ÄØ¬∑‚ÄØrecency_boost   +   0.20‚ÄØ¬∑‚ÄØassoc
    (dialogue / tool snippets keep max score)

    Top `MAX_SNIPPETS` unique snippets are kept and persisted.
    """
    import json, math, time
    from datetime import datetime, timezone
    from context import ContextObject

    # ‚îÄ‚îÄ‚îÄ tunables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    MAX_SNIPPETS        = 12
    MAX_PER_CATEGORY    = 6
    SIM_TOP_K           = max(3, getattr(self, "top_k", 3))
    HALF_LIFE_DAYS      = 3.0
    NOW_TS              = time.time()

    state = state or {}

    # ---------- helpers --------------------------------------------
    def _recency_boost(ctx) -> float:
        ts = ctx.timestamp
        try:
            if isinstance(ts, str):
                ts = datetime.fromisoformat(ts.replace("Z", "+00:00")).timestamp()
        except Exception:
            return 0.0
        age_days = max((NOW_TS - ts) / 86400.0, 0.0)
        return 0.5 ** (age_days / HALF_LIFE_DAYS)

    def _label_trim(text: str, lbl: str, limit: int = 220) -> str:
        text = text.replace("\n", " ").strip()
        if len(text) > limit:
            text = text[:limit].rsplit(" ", 1)[0] + " ‚Ä¶"
        return f"({lbl}) {text}"

    scored: list[tuple[float, str, ContextObject]] = []
    seen_texts: dict[str, float] = {}   # text ‚Üí best_score

    # ---------- 1) recent dialogue ---------------------------------
    for c in reversed(state.get("history", [])[-MAX_PER_CATEGORY:]):
        txt = c.summary or c.metadata.get("text", "")
        s   = _label_trim(txt, "USER" if c.semantic_label == "user_input" else "ASSIST")
        score = 1.0
        scored.append((score, s, c))
        seen_texts[s] = score

    # ---------- 2) recent tool outputs -----------------------------
    for c in reversed(state.get("tools", [])[-MAX_PER_CATEGORY:]):
        payload = c.metadata.get("output") or c.metadata.get("exception") or ""
        if not isinstance(payload, str):
            try:    payload = json.dumps(payload, ensure_ascii=False)[:300]
            except: payload = repr(payload)[:300]
        s = _label_trim(payload, f"TOOL:{c.stage_id}")
        score = 1.0
        scored.append((score, s, c))
        seen_texts[s] = score

    # ---------- 3) semantic & assoc recalls from previous stages ---
    for lbl, key in (("SEM", "semantic"), ("ASSOC", "assoc")):
        for c in state.get(key, [])[:MAX_PER_CATEGORY]:
            txt = c.summary or ""
            s   = _label_trim(txt, lbl)
            if s in seen_texts:
                continue
            sim = c.retrieval_score or 0.7
            scored.append((sim, s, c))
            seen_texts[s] = sim

    # ---------- 4) holographic associative recall ------------------
    seed_ids = [clar_ctx.context_id] + [c.context_id for c in state.get("history", [])[-2:]]
    assoc_hits = self.memman.holographic_recall(
        cue_ids=seed_ids,
        cue_text=clar_ctx.summary or "",
        hops=2,
        top_n=MAX_PER_CATEGORY,
        embed_fn=self.embed_text
    )
    for h in assoc_hits:
        txt = h.summary or h.metadata.get("content", "")
        s   = _label_trim(txt, "HMM")
        if s in seen_texts:
            continue
        assoc = h.retrieval_score or 0.5
        rec   = _recency_boost(h)
        score = 0.20 * assoc + 0.25 * rec + 0.55 * assoc  # assoc doubles as similarity proxy
        scored.append((score, s, h))
        seen_texts[s] = score

    # ---------- 5) fresh similarity hits (recency‚Äëboosted) ---------
    kws = clar_ctx.metadata.get("keywords") or []
    if not kws and clar_ctx.summary:
        kws = [clar_ctx.summary]

    for kw in kws:
        for h in self.engine.query(similarity_to=kw,
                                   stage_id="external_knowledge_query",
                                   top_k=SIM_TOP_K):
            txt = (h.summary or h.metadata.get("content", "")).strip()
            s   = _label_trim(txt, "FRESH")
            if s in seen_texts:
                continue
            sim  = h.retrieval_score or 0.0
            rec  = _recency_boost(h)
            score = 0.55 * sim + 0.25 * rec + 0.20 * 0.0   # no assoc for engine hits
            scored.append((score, s, h))
            seen_texts[s] = score

    # ---------- 6) final ranking & de‚Äëdup --------------------------
    scored.sort(key=lambda t: t[0], reverse=True)
    uniq_lines = []
    added = set()
    for _, txt, _ in scored:
        if txt not in added:
            uniq_lines.append(txt)
            added.add(txt)
        if len(uniq_lines) >= MAX_SNIPPETS:
            break

    # ---------- 7) persist ContextObject ---------------------------
    ext_ctx = ContextObject.make_stage(
        "external_knowledge_retrieval",
        input_refs=[clar_ctx.context_id],
        output={"snippets": uniq_lines},
    )
    ext_ctx.stage_id       = "external_knowledge_retrieval"
    ext_ctx.semantic_label = "external_knowledge"
    ext_ctx.tags.append("external")
    ext_ctx.summary = "\n".join(uniq_lines)[:1024]
    ext_ctx.touch()
    self.repo.save(ext_ctx)
    self.memman.register_relationships(ext_ctx, self.embed_text)

    # ---------- 8) debug print -------------------------------------
    self._print_stage_context(
        "external_knowledge_retrieval",
        {"chosen_snippets": uniq_lines, "total_candidates": len(scored)},
    )

    # expose snippets to downstream stages
    if state is not None:
        state["knowledge_snippets"] = uniq_lines

    return ext_ctx



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Stage‚ÄØ6 ‚Äì collect & deduplicate tool schemas
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _stage6_prepare_tools(self) -> List[Dict[str, Any]]:
    """
    Return a de-duplicated, lexicographically sorted list of:
      { "name": "<tool_name>",
        "description": "<one-line truncated desc>",
        "schema": <full JSON-RPC schema dict>
      }
    for every tool_schema in the repo.
    """
    import json, textwrap

    # 1) Load every schema context object tagged "tool_schema"
    rows = self.repo.query(
        lambda c: c.component == "schema" and "tool_schema" in c.tags
    )

    # 2) Keep only the newest per tool name
    buckets: Dict[str, Any] = {}
    for ctx in rows:
        try:
            blob = json.loads(ctx.metadata["schema"])
            name = blob["name"]
        except Exception:
            continue
        # pick the most recent
        if name not in buckets or ctx.timestamp > buckets[name].timestamp:
            buckets[name] = ctx

    # 3) Build the list, sorted by tool name, including truncated description + full schema
    tool_defs: List[Dict[str, Any]] = []
    for name in sorted(buckets):
        blob = json.loads(buckets[name].metadata["schema"])
        full_desc = blob.get("description", "").split("\n", 1)[0]
        short_desc = textwrap.shorten(full_desc, width=60, placeholder="‚Ä¶")
        tool_defs.append({
            "name":        name,
            "description": short_desc,
            "schema":      blob,
        })

    # Debug: show exactly what we're returning
    self._print_stage_context("prepare_tools_full_list", {
        "all_tool_names": [t["name"] for t in tool_defs]
    })
    return tool_defs



def _stage7_planning_summary(
    self,
    clar_ctx: ContextObject,
    know_ctx: ContextObject,
    tools_list: List[Dict[str, Any]],
    user_text: str,
    state: Dict[str, Any],
) -> Tuple[ContextObject, str]:
    """
    1) Load the latest planning_prompt artifact (or fallback to config)
    2) Inject a ‚ÄúConversation so far‚Äù block from merged context
    3) Inject truncated tool list (name + first sentence of description)
    4) Run up to 3 JSON-only planning passes, halving snippets each retry
    5) For each selected tool, refine the call against its schema
       ‚Äî in this second pass, seed with the original tool_input from
         the first pass, drop any args not in the schema, include only
         schema-defined keys, fill required params, and supply clarifier
         notes + user question so it picks real values.
    6) Persist artefacts & plan tracker; return (ctx, raw-JSON)
    """

    import json, re, hashlib, datetime
    from concurrent.futures import ThreadPoolExecutor, as_completed

    # ------------------------------------------------------------------ #
    # 0Ô∏è‚É£  Diagnostic print                                             #
    # ------------------------------------------------------------------ #
    incoming = {
        "user_text":       user_text,
        "clarifier_notes": clar_ctx.summary,
        "knowledge_snips": len((know_ctx.summary or "").splitlines()),
        "merged_len":      len(state.get("merged", [])),
        "history_len":     len(state.get("history", [])),
        "available_tools": len(tools_list),
        "semantic_len":    len(state.get("semantic", [])),
        "assoc_len":       len(state.get("assoc", [])),
    }
    banner = "=" * 20 + " PLANNER INPUT " + "=" * 20
    print("\n" + banner)
    for k, v in incoming.items():
        print(f"{k:>15}: {v}")
    print("=" * len(banner) + "\n")
    self._print_stage_context("planning_summary_tools", {
        "tool_names": [t["name"] for t in tools_list]
    })
    self._print_stage_context("planning_summary_incoming", incoming)

    # ------------------------------------------------------------------ #
    # 1Ô∏è‚É£  Build ‚ÄúConversation so far‚Äù from merged contexts             #
    # ------------------------------------------------------------------ #
    merged = state.get("merged", [])
    N = min(10, len(merged))
    convo_lines = [f"- {c.summary}" for c in merged[-N:]]
    convo_block = (
        "Conversation so far:\n" + "\n".join(convo_lines)
    ) if convo_lines else ""

    # ------------------------------------------------------------------ #
    # Helper utilities                                                  #
    # ------------------------------------------------------------------ #
    def _clean_json_block(text: str) -> str:
        m = re.search(r"```json\s*(\{.*?\})\s*```", text, flags=re.S)
        if m:
            return m.group(1)
        m2 = re.search(r"(\{.*\})", text, flags=re.S)
        return (m2.group(1) if m2 else text).strip()

    def _first_sentence(desc: str) -> str:
        head = desc.split(".", 1)[0]
        return head + ("." if not head.endswith(".") else "")

    # ------------------------------------------------------------------ #
    # 2Ô∏è‚É£  Fetch critique & prompt                                       #
    # ------------------------------------------------------------------ #
    critique_rows = sorted(
        self.repo.query(lambda c:
            c.component == "analysis" and c.semantic_label == "plan_critique"
        ),
        key=lambda c: c.timestamp,
    )
    critique_ids = [c.context_id for c in critique_rows]

    prompt_rows = sorted(
        self.repo.query(lambda c:
            c.component == "artifact" and c.semantic_label == "planning_prompt"
        ),
        key=lambda c: c.timestamp,
        reverse=True,
    )
    raw_prompt = (
        prompt_rows[0].summary
        if prompt_rows else
        self._get_prompt("planning_prompt")
    )
    first_two = ".".join(raw_prompt.split(".", 2)[:2]) + "."

    # one-line description per tool
    tool_lines = "\n".join(
        f"- **{t['name']}**: {_first_sentence(t['description'])}"
        for t in tools_list
    ) or "(none)"
    base_system = f"{first_two}\n\nAvailable tools:\n{tool_lines}"
    replan_system = (
        "Your last plan was invalid‚Äî**OUTPUT ONLY** the JSON, no extra text.\n\n"
        f"Available tools:\n{tool_lines}"
    )

    # ------------------------------------------------------------------ #
    # 3Ô∏è‚É£  Build USER message                                           #
    # ------------------------------------------------------------------ #
    original_snips = (know_ctx.summary or "").splitlines()
    def build_user(snips):
        blocks = [
            convo_block,
            f"User question:\n{user_text}",
            f"Clarified intent:\n{clar_ctx.metadata.get('notes') or clar_ctx.summary or '(none)'}",
            "Snippets:\n" + ("\n".join(snips) if snips else "(none)"),
        ]
        if state.get("plan_errors"):
            err_lines = "\n".join(f"- {e}" for e in state["plan_errors"])
            blocks.append("Previous validation errors:\n" + err_lines)
        recent = []
        for c in state.get("tools", [])[-5:]:
            ts = getattr(c, "timestamp", "")[:19].replace("T", " ")
            recent.append(f"- [{ts}] {c.stage_id}: {c.summary}")
        if recent:
            blocks.append("Recent tool outputs:\n" + "\n".join(recent))
        if state.get("plan_output_prev"):
            blocks.append("Previous plan:\n" + state["plan_output_prev"])
        if state.get("draft"):
            blocks.append("Assistant draft:\n" + state["draft"])
        return "\n\n".join(blocks)

    full_user = build_user(original_snips)

    # ------------------------------------------------------------------ #
    # 4Ô∏è‚É£  High‚Äêlevel planning loop                                    #
    # ------------------------------------------------------------------ #
    last_calls = None
    plan_obj   = None

    for attempt in range(1, 4):
        if attempt == 1:
            sys_p, user_p, tag = base_system, full_user, "[Planner]"
        else:
            keep = max(1, len(original_snips) // (2 ** (attempt - 1)))
            sys_p, user_p, tag = (
                replan_system,
                build_user(original_snips[:keep]),
                "[PlannerReplan]"
            )

        raw = self._stream_and_capture(
            self.secondary_model,
            [{"role": "system", "content": sys_p},
             {"role": "user",   "content": user_p}],
            tag=tag,
            images=state.get("images"),
        ).strip()
        cleaned = _clean_json_block(raw)

        try:
            cand = json.loads(cleaned)
        except:
            cand = None

        # normalize into {"tasks":[...]}
        if isinstance(cand, dict) and "tasks" in cand and isinstance(cand["tasks"], list):
            plan_obj = cand
        elif isinstance(cand, dict) and "tool_calls" in cand:
            tasks = []
            for tc in cand["tool_calls"]:
                if isinstance(tc, str):
                    tasks.append({"call": tc, "tool_input": {}, "subtasks": []})
                else:
                    name = tc.get("call") or tc.get("tool_call")
                    inp  = tc.get("tool_input", {}) or {}
                    subs = tc.get("subtasks", []) or []
                    tasks.append({"call": name, "tool_input": inp, "subtasks": subs})
            plan_obj = {"tasks": tasks}
        elif isinstance(cand, dict):
            plan_obj = {"tasks": [cand]}
        else:
            calls = re.findall(r"\b[A-Za-z_]\w*\([^)]*\)", cleaned or raw)
            plan_obj = {"tasks": [
                {"call": c, "tool_input": {}, "subtasks": []}
                for c in calls
            ]}

        # ensure well‚Äêformed
        if not plan_obj or not isinstance(plan_obj.get("tasks"), list):
            plan_obj = {"tasks": []}

        valid_names = {t["name"] for t in tools_list}
        if any(t.get("call") not in valid_names for t in plan_obj["tasks"]):
            continue

        calls_now = [t.get("call") for t in plan_obj["tasks"]]
        if calls_now == last_calls:
            continue
        last_calls = calls_now
        break

    # ------------------------------------------------------------------ #
    # 5Ô∏è‚É£  Refine each chosen tool with its schema                     #
    # ------------------------------------------------------------------ #
    schema_map = {t["name"]: t["schema"] for t in tools_list}

    def refine_single_tool(task: dict) -> dict:
        name     = task["call"]
        schema   = schema_map[name]
        required = schema["parameters"].get("required", [])
        props    = schema["parameters"].get("properties", {})
        schema_json = json.dumps(schema, indent=2)

        # display only this tool‚Äôs schema
        print(f"\n--- Schema for `{name}` ---\n{schema_json}\n")

        # start with the original tool_input from first pass
        refined = {
            "call":       name,
            "tool_input": dict(task.get("tool_input", {})),
            "subtasks":   list(task.get("subtasks", []))
        }
        critic = ""
        errors = []

        for retry in range(3):
            missing = [p for p in required if p not in refined["tool_input"]]
            if missing:
                errors.append(f"‚ö†Ô∏è Missing required parameters: {missing}")

            parts = []
            # seed with original clarifier/user context
            parts.append(f"Clarified intent: {clar_ctx.metadata.get('notes') or clar_ctx.summary}")
            parts.append(f"User question: {user_text}")
            if critic:
                parts.append(f"üí° Critic: {critic}")
            if errors:
                parts.append("=== ERRORS ===\n" + "\n".join(errors))
            parts.extend([
                "=== CURRENT CALL ===\n```json\n" + json.dumps({"tasks":[refined]}, indent=2) + "\n```",
                "Output **only** a JSON matching the schema exactly.",
                "‚Ä¢ Drop any args not in schema.",
                "‚Ä¢ Include all required params; do not invent extra keys.",
                "‚Ä¢ Use only these parameters and types:\n"
                + "\n".join(f"- `{k}`: {props[k]['type']}" for k in props),
                "=== SCHEMA ===\n```json\n" + schema_json + "\n```",
            ])

            out = self._stream_and_capture(
                self.secondary_model,
                [
                    {"role": "system", "content": "\n\n".join(parts)},
                    {"role": "user",   "content": json.dumps({"tasks":[refined]})},
                ],
                tag=f"[PlannerRefine_{name}]_retry{retry}",
                images=state.get("images"),
            )
            block = _clean_json_block(out)
            try:
                cand = json.loads(block)["tasks"][0]
                ti = cand.get("tool_input", {}) or {}
                # prune to schema props only
                refined["tool_input"] = {k: ti[k] for k in props if k in ti}
            except:
                pass

            if not [p for p in required if p not in refined["tool_input"]]:
                return refined

            critic = self._stream_and_capture(
                self.secondary_model,
                [
                    {"role": "system", "content":
                        "In one sentence, tell me how to fix this call to match the schema exactly."
                    },
                    {"role": "user",   "content":
                        "SCHEMA:\n" + schema_json + "\n\nCALL:\n" + json.dumps(refined)
                    },
                ],
                tag=f"[ToolCritic_{name}]",
                images=state.get("images"),
            ).strip()

        return refined

    tasks_in = plan_obj["tasks"]
    with ThreadPoolExecutor(max_workers=len(tasks_in) or 1) as pool:
        futures = [pool.submit(refine_single_tool, t) for t in tasks_in]
        refined = [f.result() for f in as_completed(futures)]

    # preserve order
    order_map = {t["call"]: t for t in refined}
    plan_obj["tasks"] = [order_map.get(t["call"], t) for t in tasks_in]

    # ------------------------------------------------------------------ #
    # 6Ô∏è‚É£  Flatten ‚Üí call_strings & record                              #
    # ------------------------------------------------------------------ #
    def _flatten(t): 
        out = [t]
        for s in t.get("subtasks", []):
            out.extend(_flatten(s))
        return out

    flat = []
    for t in plan_obj["tasks"]:
        flat.extend(_flatten(t))

    call_strings = []
    for t in flat:
        ti = t.get("tool_input", {}) or {}
        if ti:
            arg_s = ",".join(f"{k}={json.dumps(v)}" for k,v in ti.items())
            call_strings.append(f"{t['call']}({arg_s})")
        else:
            call_strings.append(f"{t['call']}()")

    state["plan_calls"]  = call_strings
    state["valid_calls"] = call_strings
    state["fixed_calls"] = call_strings

    # ------------------------------------------------------------------ #
    # 7Ô∏è‚É£  Persist artefacts & plan_tracker                            #
    # ------------------------------------------------------------------ #
    plan_json = json.dumps(plan_obj, ensure_ascii=False)
    plan_sig = hashlib.md5(plan_json.encode()).hexdigest()[:8]

    ctx = ContextObject.make_stage(
        "planning_summary",
        clar_ctx.references + know_ctx.references + critique_ids,
        {"plan": plan_obj, "attempt": attempt, "plan_id": plan_sig},
    )
    ctx.stage_id = f"planning_summary_{plan_sig}"
    ctx.summary  = plan_json
    ctx.touch()
    self.repo.save(ctx)

    succ_cls  = ContextObject.make_success if call_strings else ContextObject.make_failure
    succ_msg  = (
        f"Planner ‚Üí {len(call_strings)} task(s)"
        if call_strings else "Planner ‚Üí empty plan"
    )
    succ = succ_cls(succ_msg, refs=[ctx.context_id])
    succ.stage_id = f"planning_summary_signal_{plan_sig}"
    succ.touch()
    self.repo.save(succ)

    tracker = ContextObject.make_stage(
        "plan_tracker",
        [ctx.context_id],
        {
            "plan_id":     plan_sig,
            "plan_calls":  call_strings,
            "total_calls": len(call_strings),
            "succeeded":   0,
            "attempts":    0,
            "status":      "in_progress",
            "started_at":  datetime.datetime.utcnow().isoformat() + "Z",
        },
    )
    tracker.semantic_label = plan_sig
    tracker.stage_id       = f"plan_tracker_{plan_sig}"
    tracker.summary        = "initialized plan tracker"
    tracker.touch()
    self.repo.save(tracker)

    state["plan_ctx"]    = ctx
    state["plan_output"] = plan_json
    state["tools_list"]  = tools_list
    state["tc_ctx"]      = None

    return ctx, plan_json




def _stage7b_plan_validation(
    self,
    plan_ctx: ContextObject,
    plan_output: str,
    tools_list: List[Dict[str, str]],
    state: Dict[str, Any]
) -> Tuple[List[str], List[Tuple[str, str]], List[str]]:
    """
    Robust JSON-based plan validation. Keeps original tool_input intact,
    asks the LLM to fill in only truly missing parameters, and then
    reconstructs call-strings with full, exact arguments.

    Now *honors* the tools_list you passed in and injects full docstrings
    for just those tools you actually plan to call.
    """
    import json, re, inspect, importlib

    # A) Load all known tool schemas from your repo
    all_schemas = {
        json.loads(c.metadata["schema"])["name"]: json.loads(c.metadata["schema"])
        for c in self.repo.query(
            lambda c: c.component == "schema" and "tool_schema" in c.tags
        )
    }

    # B) Parse the planner‚Äôs JSON output
    try:
        plan_obj = json.loads(plan_output)
        tasks    = plan_obj.get("tasks", [])
    except Exception:
        raise RuntimeError("Planner output was not valid JSON; cannot validate tool_input parameters")

    # C) Figure out which tools the plan actually calls
    selected_calls = [t.get("call") for t in tasks if isinstance(t.get("call"), str)]
    available     = {t["name"] for t in tools_list}
    selected      = [name for name in selected_calls if name in available]

    # D) Build a filtered schema map for *just* those selected tools
    schemas_for_prompt = {
        name: all_schemas[name]
        for name in selected
        if name in all_schemas
    }

    # E) Inject full docstring into each filtered schema
    for name, schema in schemas_for_prompt.items():
        doc = None
        # 1) try Tools.<name>
        if hasattr(Tools, name):
            doc = inspect.getdoc(getattr(Tools, name))
        else:
            # 2) try top-level tools module
            try:
                mod = importlib.import_module("tools")
                if hasattr(mod, name):
                    doc = inspect.getdoc(getattr(mod, name))
            except ImportError:
                pass

        if doc:
            schema["description"] = doc  # overwrite truncated desc

    # F) Up to 3 repair passes: fill any missing required params
    missing = {}
    for _ in range(3):
        missing.clear()
        for idx, task in enumerate(tasks):
            name       = task.get("call")
            schema     = schemas_for_prompt.get(name)
            tool_input = task.get("tool_input", {}) or {}
            if not schema:
                continue
            required = set(schema["parameters"].get("required", []))
            found    = set(tool_input.keys())
            miss     = list(required - found)
            if miss:
                missing[idx] = miss

        if not missing:
            break

        prompt = {
            "description": "Some tool calls are missing required arguments.",
            "missing":     missing,
            "plan":        plan_obj,
            "schemas":     schemas_for_prompt
        }
        repair = self._stream_and_capture(
            self.secondary_model,
            [
                {"role": "system", "content":
                    "Return ONLY a JSON {'tasks':[...]} with each task‚Äôs tool_input now complete."},
                {"role": "user",   "content": json.dumps(prompt)},
            ],
            tag="[PlanFix]",
            images=state.get("images", None)
        ).strip()

        try:
            plan_obj = json.loads(repair)
            tasks    = plan_obj.get("tasks", [])
        except Exception:
            break

    # G) Re-serialize every task into a real call string
    fixed_calls = []
    for task in tasks:
        name = task["call"].split("(", 1)[0]            # strip if planner already added ()
        ti   = task.get("tool_input", {}) or {}
        if ti:
            args = ",".join(
                f'{k}={json.dumps(v, ensure_ascii=False)}' for k, v in ti.items()
            )
            fixed_calls.append(f"{name}({args})")
        else:
            fixed_calls.append(f"{name}()")


    # H) Persist the validation step
    meta = {
        "valid":       fixed_calls,
        "errors":      [],  
        "fixed_calls": fixed_calls
    }
    pv_ctx = ContextObject.make_stage("plan_validation", plan_ctx.references, meta)
    pv_ctx.stage_id = "plan_validation"
    pv_ctx.summary  = "OK" if not missing else f"Repaired {len(missing)} task(s)"
    #pv_ctx.touch(); self.repo.save(pv_ctx)
    self._persist_and_index([pv_ctx])
    self._print_stage_context("plan_validation", meta)

    return fixed_calls, [], fixed_calls

def _stage8_tool_chaining(
    self,
    plan_ctx: ContextObject,
    plan_output: str,
    tools_list: List[Dict[str, Any]],
    state: Dict[str, Any],
    *,
    on_token: Callable[[str], None] | None = None,
) -> Tuple[ContextObject, List[str], List[ContextObject]]:
    """
    1) Parse the plan JSON into call-strings.
    2) Substitute any tool-output placeholders from `state["last_tool_outputs"]`.
    3) Validate against known schemas.
    4) Emit a ContextObject ("tool_chaining") that records the final list.
    """
    import json, re
    from context import ContextObject
    from tools import Tools

    # Ensure Tools.repo is set
    Tools.repo = self.repo

    # Load schemas once
    try:
        all_schemas = {
            json.loads(c.metadata["schema"])["name"]: c
            for c in self.repo.query(
                lambda c: c.component == "schema" and "tool_schema" in c.tags
            )
        }
    except Exception:
        all_schemas = {}

    # Step A: Extract raw call objects from JSON plan
    calls: List[str] = []
    try:
        plan = json.loads(plan_output)
        def _collect(tasks):
            out = []
            for t in tasks or []:
                out.append(t)
                out.extend(_collect(t.get("subtasks", [])))
            return out

        for t in _collect(plan.get("tasks", [])):
            name = t.get("call", "")
            if not name:
                continue
            # JSON-encode any arguments
            kwargs = t.get("tool_input", {}) or {}
            if kwargs:
                arg_s = ",".join(
                    f"{k}={json.dumps(v, ensure_ascii=False)}"
                    for k,v in kwargs.items()
                )
                calls.append(f"{name}({arg_s})")
            else:
                calls.append(f"{name}()")
    except Exception:
        # Fallback regex
        salvage = set()
        for m in re.finditer(r"\b([A-Za-z_]\w*)\s*\(([^)]*)\)", plan_output or ""):
            fn, raw = m.group(1), m.group(2).strip()
            salvage.add(f"{fn}({raw})" if raw else f"{fn}()")
        calls = sorted(salvage)

    # Dedupe
    calls = list(dict.fromkeys(calls))

    # Step B: Placeholder substitution
    # state["last_tool_outputs"] is a dict: { "tool_name": <python obj> }
    last_out = state.get("last_tool_outputs", {})
    def _substitute( call_str: str ) -> str:
        # matches [tool.method.output] or {{tool}}
        def _replace(match):
            key = match.group(1) or match.group(2)
            val = last_out.get(key)
            return json.dumps(val, ensure_ascii=False) if val is not None else match.group(0)
        # square bracket style
        call_str = re.sub(r"\[([A-Za-z_]\w*)\.output\]", _replace, call_str)
        # moustache style
        call_str = re.sub(r"\{\{([A-Za-z_]\w*)\}\}", _replace, call_str)
        return call_str

    calls = [_substitute(c) for c in calls]

    # Step C: Schema validation / repair loop
    broken = []
    final_calls = []
    for call_str in calls:
        tool_name = call_str.split("(",1)[0]
        schema_ctx = all_schemas.get(tool_name)
        if not schema_ctx:
            # unknown tool ‚Üí drop it & record error
            broken.append(f"Unknown tool `{tool_name}`")
            continue
        final_calls.append(call_str)

    # If anything broke, bubble it up
    if broken:
        raise RuntimeError(f"Tool chaining failed: {broken}")

    # Step D: Pick schema ContextObjects
    selected_schemas = []
    for c in final_calls:
        nm = c.split("(",1)[0]
        ctx = all_schemas.get(nm)
        if ctx:
            selected_schemas.append(ctx)

    # Step E: Save the stage context
    ctx_refs = plan_ctx.references + [s.context_id for s in selected_schemas]
    tc_ctx = ContextObject.make_stage(
        "tool_chaining",
        ctx_refs,
        {"tool_calls": final_calls}
    )
    tc_ctx.stage_id = "tool_chaining"
    tc_ctx.summary  = json.dumps(final_calls, ensure_ascii=False)
    tc_ctx.touch()
    self.repo.save(tc_ctx)

    return tc_ctx, final_calls, selected_schemas

def _stage8_5_user_confirmation(
    self,
    calls: list[Any],
    user_text: str
) -> list[str]:
    """
    Surface the to-be-invoked calls for user approval.
    Auto-converts ANY function-call object Gemma might emit into the
    canonical string form `tool_name(arg1=...,arg2=...)`.
    """
    import json

    def _obj2str(item) -> str:
        # Gemma-style or OpenAI function_call object
        if isinstance(item, dict):
            name = item.get("name") or item.get("tool_name") or item.get("call")
            if not name:
                return str(item).strip()

            # accept either "arguments" or "parameters"
            args_blob = item.get("arguments", item.get("parameters", {})) or {}
            if isinstance(args_blob, dict) and args_blob:
                arg_str = ",".join(
                    f'{k}={json.dumps(v, ensure_ascii=False)}'
                    for k, v in args_blob.items()
                )
                return f"{name}({arg_str})"
            return f"{name}()"

        # already a string
        return str(item).strip()

    confirmed: list[str] = [_obj2str(c) for c in calls]

    # diagnostics
    self._print_stage_context("user_confirmation", {"calls": confirmed})

    ctx = ContextObject.make_stage(
        "user_confirmation", [], {"confirmed_calls": confirmed}
    )
    ctx.stage_id = "user_confirmation"
    ctx.summary  = f"Auto-approved: {confirmed}"
    #ctx.touch(); self.repo.save(ctx)
    self._persist_and_index([ctx])

    return confirmed


def _stage9b_reflection_and_replan(
    self,
    tool_ctxs: List[ContextObject],
    plan_output: str,
    user_text: str,
    clar_metadata: Dict[str, Any],
    state: Dict[str, Any],
    max_tokens: int = 128000
) -> Optional[str]:
    """
    Reflect on the full context (user question, clarifier notes/keywords,
    the most recent 1-3 tool outputs, original plan).  If everything satisfied the intent,
    return None; otherwise return only the corrected JSON plan.
    """
    import json, re
    from datetime import datetime

    def _parse(ts: str) -> datetime:
        if not ts:
            return datetime.min
        try:
            return datetime.fromisoformat(ts.rstrip("Z"))
        except Exception:
            try:
                return datetime.strptime(ts, "%Y%m%dT%H%M%SZ")
            except Exception:
                return datetime.min

    # 1) Gather clarifier info
    clar_notes = clar_metadata.get("notes", "")
    clar_keywords = clar_metadata.get("keywords", [])

    # 2) Build the full context blob
    parts = [
        f"=== USER QUESTION ===\n{user_text}",
        f"=== CLARIFIER NOTES ===\n{clar_notes}"
    ]
    if clar_keywords:
        parts.append(f"=== CLARIFIER KEYWORDS ===\n{', '.join(clar_keywords)}")

    # 3) Take only the most recent 1-3 tool outputs by timestamp
    sorted_tool_ctxs = sorted(
        tool_ctxs,
        key=lambda c: _parse(getattr(c, "timestamp", "") or ""),
        reverse=True
    )
    capped_tool_ctxs = sorted_tool_ctxs[:3]

    # 4) Append those tool outputs
    for c in capped_tool_ctxs:
        payload = c.metadata.get("output_short") or c.metadata.get("output_full")

        try:
            blob = json.dumps(payload, indent=2, ensure_ascii=False)
        except Exception:
            blob = repr(payload)
        parts.append(f"=== TOOL OUTPUT [{c.stage_id}] ===\n{blob}")

    # 5) Finally, the original plan
    parts.append(f"=== ORIGINAL PLAN ===\n{plan_output}")

    context_blob = "\n\n".join(parts)

    # 6) Reflection prompt
    system_msg = self._get_prompt("reflection_prompt")
    user_payload = (
        context_blob
        + "\n\nDid these tool outputs satisfy the original intent? "
          "If yes, reply OK.  If not, return only the corrected JSON plan."
    )

    msgs = [
        {"role": "system", "content": system_msg},
        {"role": "user",   "content": user_payload},
    ]
    resp = self._stream_and_capture(self.secondary_model, msgs, tag="[Reflection]").strip()

    # ‚îÄ‚îÄ NEW GUARD: identical-equals treat as OK
    try:
        new_plan = json.loads(resp)
        old_plan = json.loads(plan_output)
        if new_plan == old_plan:
            ok_ctx = ContextObject.make_success(
                description="Reflection confirmed plan (identical echo)",
                refs=[c.context_id for c in capped_tool_ctxs]
            )
            self._persist_and_index([ok_ctx])
            return None
    except:
        pass

    # ‚îÄ‚îÄ If literally "OK", keep original
    if re.fullmatch(r"(?i)(ok|okay)[.!]?", resp):
        ok_ctx = ContextObject.make_success(
            description="Reflection confirmed plan satisfied intent",
            refs=[c.context_id for c in capped_tool_ctxs]
        )
        self._persist_and_index([ok_ctx])
        return None

    # ‚îÄ‚îÄ Else record replan
    fail_ctx = ContextObject.make_failure(
        description="Reflection triggered replan",
        refs=[c.context_id for c in capped_tool_ctxs]
    )
    self._persist_and_index([fail_ctx])

    repl = ContextObject.make_stage(
        "reflection_and_replan",
        [c.context_id for c in capped_tool_ctxs],
        {"replan": resp}
    )
    repl.stage_id = "reflection_and_replan"
    repl.summary  = resp
    repl.touch(); self.repo.save(repl)

    return resp


def _stage9_invoke_with_retries(
    self,
    raw_calls: List[str],
    plan_output: str,
    selected_schemas: List[ContextObject],
    user_text: str,
    clar_metadata: Dict[str, Any],
    state: Dict[str, Any]
) -> List[ContextObject]:
    import json, re, hashlib, datetime, logging
    from tools import Tools
    # --- utilities -------------------------------------------------
    def _smart_truncate(text: str, max_len: int = 4000) -> str:
        if len(text) <= max_len:
            return text
        head = text[: max_len // 2]
        tail = text[-max_len // 2 :]
        return head + f"\n‚Ä¶ ‚ü™{len(text)-max_len} chars elided‚ü´ ‚Ä¶\n" + tail

    plan_sig = hashlib.md5(plan_output.encode("utf-8")).hexdigest()[:8]
    tracker = next(
        (c for c in self.repo.query(
            lambda c: c.component == "plan_tracker" and c.semantic_label == plan_sig
        )), None
    )

    # initialize or refresh tracker
    if not tracker:
        tracker = ContextObject.make_stage(
            "plan_tracker", [], {
                "plan_id":       plan_sig,
                "plan_calls":    raw_calls.copy(),
                "total_calls":   len(raw_calls),
                "succeeded":     0,
                "attempts":      0,
                "call_status_map": {},
                "errors_by_call": {},
                "status":        "in_progress",
                "started_at":    datetime.datetime.utcnow().isoformat() + "Z"
            }
        )
        tracker.semantic_label = plan_sig
        tracker.stage_id       = f"plan_tracker_{plan_sig}"
        tracker.summary        = "initialized plan tracker"
        tracker.touch(); self.repo.save(tracker)
    else:
        meta = tracker.metadata
        meta.setdefault("plan_calls", raw_calls.copy())
        meta.setdefault("total_calls", len(raw_calls))
        meta.setdefault("succeeded", 0)
        meta.setdefault("call_status_map", {})
        meta.setdefault("errors_by_call", {})
        meta.setdefault("status", "in_progress")
        tracker.touch(); self.repo.save(tracker)

    tracker.metadata["attempts"] = tracker.metadata.get("attempts", 0) + 1
    tracker.metadata["last_attempt_at"] = datetime.datetime.utcnow().isoformat() + "Z"
    tracker.touch(); self.repo.save(tracker)

    # normalize and helper funcs
    def _norm(calls):
        out = []
        for c in calls:
            if isinstance(c, dict) and "tool_call" in c:
                out.append(c["tool_call"])
            elif isinstance(c, str):
                out.append(c)
        return out

    def _validate(res):
        exc = res.get("exception")
        return (exc is None, exc or "")

    def normalize_key(k):
        return re.sub(r"\W+", "", k).lower()

    all_calls = _norm(raw_calls)
    call_status = tracker.metadata.setdefault("call_status_map", {})
    pending = [c for c in all_calls if not call_status.get(c, False)]
    last_results = {}
    tool_ctxs = []

    # ‚îÄ‚îÄ‚îÄ fast‚Äêpath: nothing to run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if not pending:
        tracker.metadata["status"] = "success"
        tracker.metadata["completed_at"] = datetime.datetime.utcnow().isoformat() + "Z"
        tracker.touch(); self.repo.save(tracker)

        existing = []
        for call in all_calls:
            matches = [
                c for c in self.repo.query(
                    lambda c: c.component == "tool_output"
                              and c.metadata.get("tool_call") == call
                )
            ]
            if matches:
                matches.sort(key=lambda c: c.timestamp, reverse=True)
                existing.append(matches[0])

        state["tool_ctxs"] = existing
        self.integrator.ingest(existing)
        state["merged"].extend(existing)
        return existing

    # ‚îÄ‚îÄ‚îÄ retry loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    max_retries, prev = 10, None
    for _ in range(max_retries):
        if prev is not None and pending == prev:
            logging.warning("plateau, giving up retries")
            break
        prev = pending.copy()

        import random; random.shuffle(pending)
        for original in list(pending):
            call_str = original

            # placeholder chaining
            for ph in re.findall(r"\[([^\]]+)\]", call_str):
                key = normalize_key(ph[:-7]) if ph.endswith(".output") else normalize_key(ph)
                if key in last_results:
                    call_str = call_str.replace(f"[{ph}]", repr(last_results[key]))

            # alias chaining
            for ph in re.findall(r"\{\{([^}]+)\}\}", call_str):
                phn = normalize_key(ph)
                if phn in last_results:
                    call_str = call_str.replace(f"{{{{{ph}}}}}", repr(last_results[phn]))

            # nested zero-arg calls
            for inner in re.findall(r"\b([A-Za-z_]\w*)\(\)", call_str):
                if inner not in last_results:
                    r_i = Tools.run_tool_once(f"{inner}()")
                    ok_i, err_i = _validate(r_i)
                    last_results[inner] = r_i.get("output")
                    call_str = re.sub(rf"\b{inner}\(\)", repr(last_results[inner]), call_str)
                    # persist nested result
                    try:
                        sch_i = next(
                            s for s in selected_schemas
                            if json.loads(s.metadata["schema"])["name"] == inner
                        )
                        ctx_i = ContextObject.make_stage("tool_output", [sch_i.context_id], r_i)
                        ctx_i.stage_id = f"tool_output_nested_{inner}"
                        ctx_i.summary = str(r_i.get("output")) if ok_i else f"ERROR: {err_i}"
                        ctx_i.metadata.update(r_i)
                        ctx_i.touch(); self.repo.save(ctx_i)
                        tool_ctxs.append(ctx_i)
                    except StopIteration:
                        pass

            # ‚îÄ‚îÄ‚îÄ invoke main call ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            res = Tools.run_tool_once(call_str)
            ok, err = _validate(res)
            raw_out = res.get("output")

            # pretty/short formatting
            try:
                pretty = json.dumps(raw_out, ensure_ascii=False, indent=2)
            except:
                pretty = repr(raw_out)
            short = _smart_truncate(pretty)

            # find schema ref
            name = original.split("(", 1)[0]
            refs = []
            for s in selected_schemas:
                if json.loads(s.metadata["schema"])["name"] == name:
                    refs = [s.context_id]
                    break

            # persist final tool_output
            meta = {
                "tool_call":    original,
                "output_full":  pretty,
                "output_short": short,
                "output":       raw_out,
                "exception":    res.get("exception"),
            }
            ctx = ContextObject.make_stage("tool_output", refs, meta)
            ctx.stage_id = f"tool_output_{name}"
            ctx.summary  = ("ERROR: " + str(err)) if not ok else repr(raw_out)
            ctx.touch(); self.repo.save(ctx)
            tool_ctxs.append(ctx)

            # track success/failure
            call_status[original] = ok
            tracker.metadata["succeeded"] = tracker.metadata.get("succeeded", 0) + int(ok)
            if not ok:
                tracker.metadata.setdefault("errors_by_call", {})[original] = err
            tracker.touch(); self.repo.save(tracker)

            if ok:
                pending.remove(original)

        if not pending:
            # ‚îÄ‚îÄ‚îÄ on successful retry ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            tracker.metadata["status"] = "success"
            tracker.metadata["completed_at"] = datetime.datetime.utcnow().isoformat() + "Z"
            self.repo.save(tracker)

            existing = []
            for call in all_calls:
                matches = [
                    c for c in self.repo.query(
                        lambda c: c.component == "tool_output"
                                  and c.metadata.get("tool_call") == call
                    )
                ]
                if matches:
                    matches.sort(key=lambda c: c.timestamp, reverse=True)
                    existing.append(matches[0])

            state["tool_ctxs"] = existing
            self.integrator.ingest(existing)    # ‚òÖ ensure integrator knows them
            state["merged"].extend(existing)    # ‚òÖ and Stage10 sees them
            return existing

        # if still pending, abort retry loop
        break

    # ‚îÄ‚îÄ‚îÄ final fail ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    tracker.metadata["status"] = "failed"
    tracker.metadata["last_errors"] = list(tracker.metadata.get("errors_by_call", {}).values())
    tracker.metadata["completed_at"] = datetime.datetime.utcnow().isoformat() + "Z"
    tracker.touch(); self.repo.save(tracker)

    state["tool_ctxs"] = tool_ctxs
    self.integrator.ingest(tool_ctxs)
    state["merged"].extend(tool_ctxs)
    return tool_ctxs

def _await_if_needed(obj):
    """Return result synchronously whether obj is a coroutine or not.
       Safe to call from a worker thread (no running loop there)."""
    if inspect.isawaitable(obj):
        return asyncio.run(obj)   # we're inside a to_thread worker, so no active loop
    return obj


def _stage10_assemble_and_infer(self, user_text: str, state: dict[str, Any]) -> str:
    import json, pprint, logging
    from collections import OrderedDict
    from context import ContextObject
    from types import SimpleNamespace
    from datetime import datetime, timedelta

    logger = logging.getLogger(__name__)

    # ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _parse(ts: str) -> datetime:
        if not ts:
            return datetime.min
        try:
            return datetime.fromisoformat(ts.rstrip("Z"))
        except Exception:
            try:
                return datetime.strptime(ts, "%Y%m%dT%H%M%SZ")
            except Exception:
                return datetime.min

    def normalize_tc(raw):
        if isinstance(raw, dict):
            meta = raw.get("metadata", {}) or {}
            stage_id = raw.get("stage_id") or raw.get("call") or raw.get("tool_name", "unknown_tool")
            context_id = raw.get("context_id", stage_id)
            timestamp = ""
            result = raw.get("result", raw.get("output", {}))
            if isinstance(result, dict):
                timestamp = result.get("timestamp", "")
            summary = raw.get("summary", None)
            if summary is None:
                try:
                    summary = json.dumps(result, ensure_ascii=False)
                except Exception:
                    summary = pprint.pformat(result)
            return SimpleNamespace(
                metadata=meta if isinstance(meta, dict) else {},
                summary=summary,
                stage_id=stage_id,
                timestamp=timestamp,
                context_id=context_id,
            )
        else:
            meta = getattr(raw, "metadata", {}) or {}
            stage_id = getattr(raw, "stage_id", None) or getattr(raw, "stage", None) or "unknown_tool"
            context_id = getattr(raw, "context_id", stage_id)
            timestamp = getattr(raw, "timestamp", "")
            summary = getattr(raw, "summary", None)
            if summary is None:
                candidate = meta.get("output", meta.get("output_full", None))
                if candidate is None:
                    summary = ""
                elif isinstance(candidate, (dict, list)):
                    try:
                        summary = json.dumps(candidate, ensure_ascii=False)
                    except Exception:
                        summary = pprint.pformat(candidate)
                else:
                    summary = str(candidate)
            return SimpleNamespace(
                metadata=meta if isinstance(meta, dict) else {},
                summary=summary,
                stage_id=stage_id,
                timestamp=timestamp,
                context_id=context_id,
            )

    # ‚îÄ‚îÄ‚îÄ 1) Conversation snippets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    merged = state.get("merged", []) or []

    # ‚îÄ‚îÄ‚îÄ 2) Tool outputs: grab only most recent 1‚Äì3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    raw_tool_ctxs = []
    try:
        repo_tool_ctxs = list(self.repo.query(lambda c: c.semantic_label == "tool_output"))
    except Exception:
        repo_tool_ctxs = []
    # sort newest first by timestamp
    repo_tool_ctxs_sorted = sorted(
        repo_tool_ctxs,
        key=lambda c: _parse(getattr(c, "timestamp", "") or ""),
        reverse=True
    )
    if repo_tool_ctxs_sorted:
        raw_tool_ctxs = repo_tool_ctxs_sorted[:3]
    else:
        raw_tool_ctxs = (state.get("tool_ctxs", []) or [])[-3:]
        if not raw_tool_ctxs:
            tool_summaries = state.get("tool_summaries", []) or []
            for summ in tool_summaries[-3:]:
                call_name = summ.get("call") or summ.get("tool_name", "unknown_tool")
                result = summ.get("result")
                ts = ""
                if isinstance(result, dict):
                    ts = result.get("timestamp", "")
                tc = {
                    "metadata": {"output": result},
                    "summary": json.dumps(result, ensure_ascii=False, indent=2),
                    "stage_id": call_name,
                    "timestamp": ts,
                    "context_id": summ.get("context_id", call_name),
                }
                raw_tool_ctxs.append(tc)
            raw_tool_ctxs = raw_tool_ctxs[-3:]

    tool_ctxs = []
    for raw in raw_tool_ctxs:
        try:
            norm = normalize_tc(raw)
            tool_ctxs.append(norm)
        except Exception as e:
            logger.exception("Failed to normalize tool context entry: %s", e)

    # dynamic recent context size based on number of tools captured (fallback to 32 if none)
    num_tools = len(tool_ctxs)
    MAX_CTX_OBJS = num_tools if num_tools > 0 else 32
    recent = merged[-MAX_CTX_OBJS:]
    convo_lines = [getattr(c, "summary", "") or "" for c in recent]
    conversation_block = (
        "[Conversation so far]\n" + "\n".join(convo_lines)
        if convo_lines else ""
    )

    # build tools_block
    tool_blocks = []
    for tc in tool_ctxs:
        meta = getattr(tc, "metadata", {}) or {}
        data = meta.get("output", meta.get("output_full", None))
        if data is None:
            data = tc.summary or ""
        try:
            if isinstance(data, (dict, list)):
                payload = json.dumps(data, ensure_ascii=False, indent=2)
            else:
                payload = str(data)
        except Exception:
            payload = pprint.pformat(data, compact=True)
        call_name = meta.get("tool_call") or getattr(tc, "stage_id", "unknown_tool")
        ts = getattr(tc, "timestamp", "")
        tool_blocks.append(f"--- {getattr(tc, 'stage_id', 'unknown_tool')} ({call_name}) @ {ts} ---")
        tool_blocks.append(payload)
    tools_block = "[Tool outputs]\n" + "\n\n".join(tool_blocks) if tool_blocks else ""

    # ‚îÄ‚îÄ‚îÄ 3) Narrative block ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    narr_ctx = self._load_narrative_context()
    narrative_block = "[Narrative]\n" + (getattr(narr_ctx, "summary", "") or "")

    # ‚îÄ‚îÄ‚îÄ 4) Recent-history bullets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    recent_hist = recent[-5:]
    bullets = [f"- {getattr(c, 'semantic_label', '')}: {getattr(c, 'summary', '')}" for c in recent_hist]
    recent_hist_block = "[Recent History]\n" + "\n".join(bullets) if bullets else ""

    # ‚îÄ‚îÄ‚îÄ 5) Plan & user question ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    raw_plan = state.get("plan_output", "(no plan)")
    if not isinstance(raw_plan, str):
        try:
            raw_plan = json.dumps(raw_plan, ensure_ascii=False, indent=2)
        except Exception:
            raw_plan = pprint.pformat(raw_plan, compact=True)
    plan_block = "[Plan]\n" + raw_plan
    user_block = "[User question]\n" + user_text

    # ‚îÄ‚îÄ‚îÄ 6) Compose LLM messages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    final_sys = self._get_prompt("final_inference_prompt")
    msgs = [{"role": "system", "content": final_sys}]
    if conversation_block:
        msgs.append({"role": "system", "content": conversation_block})
    if recent_hist_block:
        msgs.append({"role": "system", "content": recent_hist_block})
    msgs.append({"role": "system", "content": plan_block})
    msgs.append({"role": "system", "content": user_block})
    msgs.append({"role": "user", "content": user_text})
    if tools_block:
        msgs.append({"role": "system", "content": tools_block})

    # ‚îÄ‚îÄ‚îÄ 7) DEBUG payload ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    merged_ids = [getattr(c, "context_id", None) for c in recent]
    tool_ctx_ids = [getattr(tc, "context_id", getattr(tc, "stage_id", "unknown_tool")) for tc in tool_ctxs]
    debug_payload = OrderedDict([
        ("conversation_block", conversation_block),
        ("tools_block",        tools_block),
        ("narrative_block",    narrative_block),
        ("recent_hist_block",  recent_hist_block),
        ("plan_block",         plan_block),
        ("user_block",         user_block),
        ("assembled_msgs",     json.dumps(msgs, ensure_ascii=False, indent=2)),
        ("merged_ids",         merged_ids),
        ("tool_ctx_ids",       tool_ctx_ids),
    ])
    self._print_stage_context("assemble_and_infer", debug_payload)

    # ‚îÄ‚îÄ‚îÄ 8) Call the model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    raw_reply = _await_if_needed(
        self._stream_and_capture(
            self.primary_model,
            msgs,
            tag="[Assistant]",
            images=state.get("images")
        )
    )
    reply = (raw_reply or "").strip()

    # ‚îÄ‚îÄ‚îÄ 9) Persist assistant reply ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    refs = merged_ids + tool_ctx_ids
    resp_ctx = ContextObject.make_stage("final_inference", refs, {"text": reply})
    resp_ctx.stage_id = "final_inference"
    resp_ctx.summary = reply
    self._persist_and_index([resp_ctx])

    seg = ContextObject.make_segment("assistant", [resp_ctx.context_id], tags=["assistant"])
    seg.summary = reply
    seg.stage_id = "assistant"
    seg.touch()
    self.repo.save(seg)

    state["draft"] = reply
    state["assistant_ctx"] = resp_ctx
    return reply


def _stage10b_response_critique_and_safety(
    self,
    draft: str,
    user_text: str,
    tool_ctxs: list["ContextObject"],
    state: dict[str, Any],
) -> str:
    import json, difflib, pprint, logging
    from context import ContextObject
    from types import SimpleNamespace
    from datetime import datetime

    logger = logging.getLogger(__name__)

    if not draft:
        return draft

    # ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _parse(ts: str) -> datetime:
        if not ts:
            return datetime.min
        try:
            return datetime.fromisoformat(ts.rstrip("Z"))
        except Exception:
            try:
                return datetime.strptime(ts, "%Y%m%dT%H%M%SZ")
            except Exception:
                return datetime.min

    def normalize_tc(raw):
        if isinstance(raw, dict):
            meta = raw.get("metadata", {}) or {}
            stage_id = raw.get("stage_id") or raw.get("call") or raw.get("tool_name", "unknown_tool")
            context_id = raw.get("context_id", stage_id)
            timestamp = ""
            result = raw.get("result", raw.get("output", {}))
            if isinstance(result, dict):
                timestamp = result.get("timestamp", "")
            summary = raw.get("summary", None)
            if summary is None:
                try:
                    summary = json.dumps(result, ensure_ascii=False)
                except Exception:
                    summary = pprint.pformat(result)
            return SimpleNamespace(
                metadata=meta if isinstance(meta, dict) else {},
                summary=summary,
                stage_id=stage_id,
                timestamp=timestamp,
                context_id=context_id,
            )
        else:
            meta = getattr(raw, "metadata", {}) or {}
            stage_id = getattr(raw, "stage_id", None) or getattr(raw, "stage", None) or "unknown_tool"
            context_id = getattr(raw, "context_id", stage_id)
            timestamp = getattr(raw, "timestamp", "")
            summary = getattr(raw, "summary", None)
            if summary is None:
                candidate = meta.get("output", meta.get("output_full", None))
                if candidate is None:
                    summary = ""
                elif isinstance(candidate, (dict, list)):
                    try:
                        summary = json.dumps(candidate, ensure_ascii=False)
                    except Exception:
                        summary = pprint.pformat(candidate)
                else:
                    summary = str(candidate)
            return SimpleNamespace(
                metadata=meta if isinstance(meta, dict) else {},
                summary=summary,
                stage_id=stage_id,
                timestamp=timestamp,
                context_id=context_id,
            )

    # ‚îÄ‚îÄ‚îÄ 1) Gather real ContextObjects or fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    raw_tool_ctxs = tool_ctxs or []
    if not raw_tool_ctxs:
        try:
            raw_tool_ctxs = list(self.repo.query(lambda c: c.semantic_label == "tool_output"))
        except Exception:
            raw_tool_ctxs = []

    if not raw_tool_ctxs:
        raw_tool_ctxs = state.get("tool_ctxs", []) or []

    if not raw_tool_ctxs:
        for summ in state.get("tool_summaries", []):
            call_name = summ.get("call") or summ.get("tool_name", "unknown_tool")
            result = summ.get("result")
            ts = ""
            if isinstance(result, dict):
                ts = result.get("timestamp", "")
            tc = {
                "metadata": {"output": result},
                "summary": json.dumps(result, ensure_ascii=False, indent=2),
                "stage_id": call_name,
                "timestamp": ts,
                "context_id": summ.get("context_id", call_name)
            }
            raw_tool_ctxs.append(tc)

    # ‚îÄ‚îÄ‚îÄ 2) Sort and cap to most recent up to 3 tool outputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    normalized = []
    for raw in raw_tool_ctxs:
        try:
            normalized.append(normalize_tc(raw))
        except Exception as e:
            logger.exception("Failed to normalize tool context entry: %s", e)
    normalized.sort(key=lambda c: _parse(getattr(c, "timestamp", "") or ""), reverse=True)
    MAX_TOOL_CTXS = 3
    real_tool_ctxs = normalized[:MAX_TOOL_CTXS]

    # ‚îÄ‚îÄ‚îÄ 3) Full merged context text ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    merged_ctxs = state.get("merged", []) or []
    merged_texts = "\n\n".join(f"[{getattr(c, 'stage_id', '')}] {getattr(c, 'summary', '')}" for c in merged_ctxs)

    # ‚îÄ‚îÄ‚îÄ 4) Plan text ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    plan_text = state.get("plan_output", "(no plan)")

    # ‚îÄ‚îÄ‚îÄ 5) Serialize selected (capped) tool outputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    outputs = []
    for c in real_tool_ctxs:
        raw = getattr(c, "metadata", {}) or {}
        # prefer structured output
        candidate = raw.get("output") if isinstance(raw, dict) else None
        if candidate is None:
            candidate = raw.get("output_full", raw) if isinstance(raw, dict) else raw
        if isinstance(candidate, dict) and "results" in candidate:
            fragment = "\n".join(f"{r.get('timestamp','')} {r.get('role','')}: {r.get('content','')}"
                                 for r in candidate["results"])
        else:
            try:
                fragment = json.dumps(candidate, indent=2, ensure_ascii=False)
            except Exception:
                fragment = pprint.pformat(candidate, compact=True)
        outputs.append(f"[{getattr(c, 'stage_id', '')}]\n{fragment}")

    # ‚îÄ‚îÄ‚îÄ 6) Relevance Extraction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    extractor_sys = self._get_prompt("extractor_sys_prompt")
    extractor_user = "\n\n".join([
        f"USER QUESTION:\n{user_text}",
        f"PLAN EXECUTED:\n{plan_text}",
        "MERGED CONTEXT SNIPPETS:\n" + (merged_texts or "(none)"),
        "RAW TOOL OUTPUTS (most recent up to 3):\n" + ("\n\n".join(outputs) or "(none)"),
        "DRAFT RESPONSE:\n" + draft,
        "Your goal: identify exactly the facts, data points, or insights that should shape the revised response."
    ])
    bullets = self._stream_and_capture(
        self.secondary_model,
        [
            {"role": "system", "content": extractor_sys},
            {"role": "user",   "content": extractor_user},
        ],
        tag="[RelevExtract]",
        images=state.get("images"),
    ).strip()

    sum_ctx = ContextObject.make_stage("relevance_summary", [], {"summary": bullets})
    sum_ctx.stage_id = "relevance_summary"
    sum_ctx.summary = bullets
    self._persist_and_index([sum_ctx])

    # ‚îÄ‚îÄ‚îÄ 7) Polishing / Safety Critique ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    editor_sys = self._get_prompt("editor_sys_prompt")
    editor_user = "\n\n".join([
        f"USER QUESTION:\n{user_text}",
        "RELEVANCE BULLETS:\n" + bullets,
        "CURRENT DRAFT:\n" + draft
    ])
    polished = self._stream_and_capture(
        self.secondary_model,
        [
            {"role": "system", "content": editor_sys},
            {"role": "user",   "content": editor_user},
        ],
        tag="[Polisher]",
        images=state.get("images"),
    ).strip()

    if polished == draft.strip():
        return polished

    # ‚îÄ‚îÄ‚îÄ 8) diff & dynamic patch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    diff = difflib.unified_diff(draft.splitlines(), polished.splitlines(), lineterm="", n=1)
    diff_summary = "; ".join(ln for ln in diff if ln.startswith(("+ ", "- "))) or "(format refined)"

    patch_rows = self.repo.query(
        lambda c: c.component == "policy" and c.semantic_label == "dynamic_prompt_patch"
    )
    patch_rows.sort(key=lambda c: getattr(c, "timestamp", ""), reverse=True)
    dynamic_patch = patch_rows[0] if patch_rows else ContextObject.make_policy(
        "dynamic_prompt_patch", diff_summary, tags=["dynamic_prompt"]
    )
    if dynamic_patch.summary != diff_summary:
        dynamic_patch.summary = diff_summary
        dynamic_patch.metadata["policy"] = diff_summary
        dynamic_patch.touch(); self.repo.save(dynamic_patch)

    resp_ctx = ContextObject.make_stage("response_critique", [sum_ctx.context_id], {"text": polished})
    resp_ctx.stage_id = "response_critique"
    resp_ctx.summary = polished
    self._persist_and_index([resp_ctx])

    critique_ctx = ContextObject.make_stage(
        "plan_critique",
        [resp_ctx.context_id] + [c.context_id for c in real_tool_ctxs],
        {"critique": polished, "diff": diff_summary},
    )
    critique_ctx.component = "analysis"
    critique_ctx.semantic_label = "plan_critique"
    self._persist_and_index([critique_ctx])

    return polished




def _stage11_memory_writeback(
    self,
    final_answer: str,
    tool_ctxs: list[ContextObject],
) -> None:
    """
    Long-term memory write-back that never balloons context.jsonl.

    ‚Ä¢ `auto_memory` ‚Üí *singleton* (insert once, then update in-place)
    ‚Ä¢ narrative     ‚Üí one new row per turn (intended)
    ‚Ä¢ every object is persisted exactly ONCE
    """

    # ‚îÄ‚îÄ 1)  Up-sert the single `auto_memory` row ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    mem_candidates = self.repo.query(
        lambda c: c.domain == "artifact"
        and c.component == "knowledge"
        and c.semantic_label == "auto_memory"
    )
    mem = mem_candidates[0] if mem_candidates else None

    if mem is None:                             # first run  ‚Üí INSERT
        mem = ContextObject.make_knowledge(
            label   = "auto_memory",
            content = final_answer,
            tags    = ["memory_writeback"],
        )
    else:                                       # later runs ‚Üí UPDATE (if text changed)
        if mem.metadata.get("content") != final_answer:
            mem.metadata["content"] = final_answer
            mem.summary             = final_answer

    mem.touch()                                 # refresh timestamp / last_accessed

    # IMPORTANT:  call reinforce **before** the single save below.
    # MemoryManager mutates mem in-place but does NOT append a new row,
    # so persisting once afterwards keeps the file tidy.
    # ‚îÄ‚îÄ Guard against dangling refs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    valid_refs = []
    for c in tool_ctxs:
        try:
            # verify the object still exists (and is persisted)
            self.repo.get(c.context_id)
            valid_refs.append(c.context_id)
        except KeyError:
            # skip IDs that were deduped, pruned, or never saved
            continue

    self.memman.reinforce(mem.context_id, valid_refs)

    # One narrative row per *unique* answer ‚Äì duplicates are skipped
    narr = ContextObject.make_narrative(
        f"At {default_clock().strftime('%Y-%m-%d %H:%M:%SZ')}, "
        f"I handled the user‚Äôs request and generated: "
        f"{final_answer[:200]}‚Ä¶"
    )
    # make_narrative() already touches & saves when it reuses a row;
    # only save when we truly inserted a new one
    if narr.context_id not in {c.context_id for c in self.repo.query(lambda c: c.component == "narrative")}:
        narr.touch()
        self.repo.save(narr)

def _stage_generate_narrative(self, state: Dict[str, Any]) -> ContextObject:
    """
    Build a running narrative of this conversation turn by turn,
    link it to all the context objects we‚Äôve touched so far,
    and store the narrative‚Äôs ContextObject ID for future reference.
    """
    #‚îÄ‚îÄ only once ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if getattr(self, "_narrative_emitted", False):
        return self._narrative_cache

    # gather all the IDs of contexts created/used this turn
    used_ids = []
    for key in ("user_ctx","sys_ctx","clar_ctx","know_ctx","plan_ctx","tc_ctx"):
        if key in state:
            used_ids.append(state[key].context_id)
    used_ids += [c.context_id for c in state.get("tool_ctxs",[])]
    # de-dupe
    used_ids = list(dict.fromkeys(used_ids))

    # assemble human narrative
    from datetime import datetime
    lines = [
        f"{datetime.utcnow():%Y-%m-%d %H:%M:%SZ}:",
        f"‚Ä¢ User asked: {state['user_text']!r}",
        f"‚Ä¢ Clarified into: {state['clar_ctx'].summary!r}",
    ]
    if "plan_output" in state:
        lines.append(f"‚Ä¢ Planner proposed: {state['plan_output']}")
    if "final" in state:
        lines.append(f"‚Ä¢ Assistant replied: {state['final']!r}")

    narrative_text = "\n".join(lines)

    # upsert the single narrative_context keeper
    nc = self._get_or_make_singleton(
        label="narrative_context",
        component="stage",
        tags=["narrative"]
    )
    nc.metadata.setdefault("history_ids", []).extend(used_ids)
    nc.metadata["history_text"] = (
        (nc.metadata.get("history_text","") + "\n\n" + narrative_text)
        .strip()
    )
    nc.summary    = nc.metadata["history_text"]
    nc.references = nc.metadata["history_ids"]

    nc.touch()
    self.repo.save(nc)

    # mark done and cache
    self._narrative_emitted = True
    self._narrative_cache   = nc
    return nc

    
def _stage_prune_context_store(self, state: Dict[str, Any]) -> str:
    """
    Remove *only* ephemeral contexts older than `context_ttl_days`
    or beyond a hard cap, leaving static prompts/schemas untouched.
    Returns a one-line summary for status_cb.
    """
    from datetime import datetime, timedelta

    cutoff = default_clock() - timedelta(days=self.context_ttl_days)
    EPHEMERAL = {
        "segment", "tool_output", "knowledge", "narrative", "stage_performance"
    }

    deleted = 0
    # 1) Delete by age
    for ctx in self.repo.query(lambda c: c.component in EPHEMERAL):
        ts_raw = ctx.timestamp.rstrip("Z") if isinstance(ctx.timestamp, str) else None
        try:
            ts = datetime.fromisoformat(ts_raw) if ts_raw else None
        except Exception:
            continue
        if ts and ts < cutoff:
            try:
                self.repo.delete(ctx.context_id)
                deleted += 1
            except KeyError:
                pass

    # 2) Hard cap
    all_ephe = [
        c for c in self.repo.query(lambda c: c.component in EPHEMERAL)
    ]
    all_ephe.sort(
        key=lambda c: datetime.fromisoformat(
            (c.timestamp.rstrip("Z") if isinstance(c.timestamp, str) else default_clock().isoformat())
        ),
        reverse=True
    )
    cap = self.cfg.get("max_total_context", 1000)
    for old_ctx in all_ephe[cap:]:
        try:
            self.repo.delete(old_ctx.context_id)
            deleted += 1
        except KeyError:
            pass

    return f"pruned {deleted} items (ttl={self.context_ttl_days}d, cap={cap})"

def _stage_narrative_mull(self, state: Dict[str, Any]) -> str:
    """
    Async ‚Äúself-talk‚Äù that:
        1. Gathers narrative, prompts, architecture.
        2. Pulls last-turn stage metrics & tool failures.
        3. Asks the LLM for ‚â§3 improvement items (diagnosis + questions + patches + mini-plans).
        4. Records Q&A, applies prompt patches, executes any plan_calls via normal pipeline.
    """
    import threading, io, contextlib, json, textwrap, datetime

    def _arch_dump() -> str:
        buf = io.StringIO()
        with contextlib.redirect_stdout(buf):
            self.dump_architecture()
        return buf.getvalue()

    def _collect_metrics() -> str:
        # fetch all stage_performance objects from repo
        perf_rows = list(self.repo.query(lambda c: c.component=="stage_performance"))
        data = [
            {
                "stage": r.metadata["stage"],
                "duration": round(r.metadata["duration"],3),
                "error": r.metadata["error"]
            }
            for r in perf_rows[-20:]  # last 20 entries
        ]
        return json.dumps(data, indent=2)

    def _collect_tool_failures() -> str:
        failures = []
        for ctx in state.get("tool_ctxs", []):
            if ctx.output.get("result", "").startswith("ERROR"):
                failures.append({
                    "call": ctx.metadata.get("call"),
                    "error": ctx.output["result"]
                })
        return json.dumps(failures, indent=2)

    def _worker():
        try:
            # 1) narrative + prompts + arch
            narr = self._load_narrative_context()
            full_narr = narr.metadata.get("history_text", narr.summary or "")

            prompts = self.repo.query(
                lambda c: c.component in ("prompt","policy") and "dynamic_prompt" not in c.tags
            )
            prompts.sort(key=lambda c: c.timestamp)
            prompt_block = "\n".join(
                f"- {textwrap.shorten(p.summary or '', 80)}"
                for p in prompts
            ) or "(none)"

            arch = _arch_dump()
            metrics = _collect_metrics()
            fails  = _collect_tool_failures()

            # 2) assemble meta-prompt
            meta = (
                self._get_prompt("narrative_mull_prompt")
                + "\n\n### Narrative ###\n" + full_narr
                + "\n\n### Prompts ###\n" + prompt_block
                + "\n\n### Architecture ###\n" + arch
                + "\n\n### Recent Stage Metrics ###\n" + metrics
                + "\n\n### Tool Failures ###\n" + fails
            )

            raw = self._stream_and_capture(
                self.primary_model,
                [{"role":"system","content": meta}],
                tag="[NarrativeMull]"
            ).strip()
            data = json.loads(raw)
            issues = data.get("issues", [])
            if not isinstance(issues, list):
                return

        except Exception:
            return  # abort silently

        # 3) process each issue
        for idx, item in enumerate(issues, 1):
            if not isinstance(item, dict):
                continue
            area      = item.get("area", f"area_{idx}")
            diag      = item.get("diagnosis","").strip()
            q_text    = item.get("question","").strip()
            patch     = item.get("prompt_patch","").strip()
            plan_calls= item.get("plan_calls", [])

            # record question
            q_ctx = ContextObject.make_stage(
                "narrative_question",
                [narr.context_id],
                {"area": area, "diagnosis": diag, "question": q_text}
            )
            q_ctx.component="narrative"; q_ctx.tags.append("narrative")
            #q_ctx.touch(); self.repo.save(q_ctx)
            self._persist_and_index([q_ctx])

            # get answer
            answer = ""
            if q_text:
                answer = self._stream_and_capture(
                    self.primary_model,
                    [
                        {"role":"system","content":
                            "You are the same meta-reasoner; answer only from the given data, be concise."},
                        {"role":"user","content": q_text}
                    ],
                    tag=f"[NarrativeAnswer_{idx}]",
                    images=state.get("images")
                ).strip()

            # record answer
            a_ctx = ContextObject.make_stage(
                "narrative_answer",
                [q_ctx.context_id],
                {"answer": answer}
            )
            a_ctx.component="narrative"; a_ctx.tags.append("narrative")
            #a_ctx.touch(); self.repo.save(a_ctx)
            self._persist_and_index([a_ctx])

            # apply prompt patch
            if patch:
                txt = (
                    f"// {datetime.datetime.utcnow().isoformat()}Z\n"
                    f"Issue:{area}\nDiag:{diag}\nQ:{q_text}\nA:{answer}\nPATCH:{patch}\n"
                )
                dyn = ContextObject.make_policy(
                    "dynamic_prompt_patch", policy_text=txt, tags=["dynamic_prompt"]
                )
                dyn.touch(); self.repo.save(dyn)

            # run plan_calls via real pipeline
            if plan_calls:
                try:
                    plan_obj = {"tasks":[
                        {"call":c.split("(",1)[0], "tool_input":{}, "subtasks": []}
                        for c in plan_calls
                    ]}
                    pj = json.dumps(plan_obj)
                    p_ctx = ContextObject.make_stage("internal_plan",[a_ctx.context_id],{"plan":plan_obj})
                    #p_ctx.touch(); self.repo.save(p_ctx)
                    self._persist_and_index([p_ctx])

                    tools = self._stage6_prepare_tools()
                    fixed,_,_ = self._stage7b_plan_validation(p_ctx,pj,tools)
                    tc, calls, schemas = self._stage8_tool_chaining(p_ctx,"\n".join(fixed),tools)
                    self._stage9_invoke_with_retries(
                        raw_calls=calls, plan_output="\n".join(fixed),
                        selected_schemas=schemas,
                        user_text="(self-review)", clar_metadata={}
                    )
                except Exception as e:
                    err = ContextObject.make_failure(
                        f"narrative_mull plan error: {e}", refs=[a_ctx.context_id]
                    )
                    err.touch(); self.repo.save(err)

    # start thread
    threading.Thread(target=_worker, daemon=True).start()
    return "(narrative mull dispatched)"